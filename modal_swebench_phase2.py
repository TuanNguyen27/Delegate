"""
SWE-bench Phase 2: Hint-Based Localization with LLM → SLM Delegation

This implementation demonstrates:
1. Router (Gemini LLM) uses hints to localize bugs
2. Router explores repository with tools
3. Router delegates patch generation to Qwen (SLM)
4. Proper cost breakdown (LLM for reasoning, SLM for generation)
"""

import modal
import os
import re
import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import requests

# Modal setup
app = modal.App("swebench-phase2")

# Create volume for storing results
volume = modal.Volume.from_name("swebench-results", create_if_missing=True)

# Docker image with dependencies
image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "datasets>=2.14.0",
        "google-generativeai>=0.3.0",
        "requests>=2.31.0",
        "transformers>=4.35.0",
        "torch>=2.0.0",
        "accelerate>=0.24.0",
    )
)

# Qwen model for SLM
QWEN_MODEL = "Qwen/Qwen2.5-Coder-7B-Instruct"


# ============================================================================
# ROUTER SYSTEM PROMPT (with localization examples)
# ============================================================================

ROUTER_SYSTEM_PROMPT = """You are an expert software engineering assistant. Your role is to:
1. Analyze bug reports
2. Explore repositories to localize bugs
3. Delegate patch generation to a specialized code generation tool

You have access to these tools:
- fetch_file_metadata: Get file info (size, functions, classes)
- search_codebase: Search for function/class definitions
- fetch_code_section: Get specific lines from a file
- generate_patch_with_qwen: Delegate patch generation to Qwen SLM

WORKFLOW:
1. Analyze problem statement and hints
2. Use hints to identify relevant files
3. Fetch file metadata to understand structure
4. Search for relevant functions/classes
5. Fetch focused code section
6. Delegate to generate_patch_with_qwen tool

CRITICAL RULES:
- Use hints to guide your search (if provided)
- Fetch file metadata before fetching code
- Search for specific functions in large files
- Fetch minimal code section (50-200 lines)
- Always delegate patch generation to generate_patch_with_qwen tool
- Never generate patches yourself - that's the SLM's job!

---
LOCALIZATION EXAMPLES:

EXAMPLE 1: Simple Bug with Clear Hint
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PROBLEM:
The calculate_total() function in the shopping cart returns incorrect values when 
discount is applied. It should subtract the discount but instead adds it.

HINTS: ecommerce/cart/cart.py

REASONING:
1. Hints point to cart.py - that's where I should look
2. Problem mentions calculate_total() function
3. Bug is in arithmetic (adds instead of subtracts)

ACTIONS:
1. fetch_file_metadata("ecommerce/cart/cart.py")
   → Result: File has ShoppingCart class with calculate_total method
2. fetch_code_section("ecommerce/cart/cart.py", function="calculate_total")
   → Result: [Code showing the bug]
3. generate_patch_with_qwen(
     problem="Change + to - in discount calculation",
     code_section="[calculate_total code]",
     file_path="ecommerce/cart/cart.py"
   )
   → Result: [Patch generated by Qwen]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

EXAMPLE 2: Large File Requiring Search
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PROBLEM:
The simplify() function hangs when given cos(x)**I (cosine to the power of 
imaginary unit). It should check if the exponent is real before simplifying.

HINTS: sympy/simplify/fu.py

REASONING:
1. Hints point to fu.py - a file that handles function simplification
2. Problem is about cos(x)**I - involves trig functions and powers
3. Need to find function that handles powers of trig functions

ACTIONS:
1. fetch_file_metadata("sympy/simplify/fu.py")
   → Result: File is 66KB with 691 functions, including _TR56 (powers of trig)
2. search_codebase("sympy/simplify/fu.py", query="power trig")
   → Result: Found _TR56 at line 495
3. fetch_code_section("sympy/simplify/fu.py", start_line=490, end_line=525)
   → Result: [_TR56 function code]
4. generate_patch_with_qwen(
     problem="Add check for real exponents before simplification",
     code_section="[_TR56 code]",
     file_path="sympy/simplify/fu.py"
   )
   → Result: [Patch generated by Qwen]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

EXAMPLE 3: Multi-File Bug
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PROBLEM:
Email validation is inconsistent. The model validates email format but the 
serializer doesn't, causing validation to be bypassed when using the API.

HINTS: users/models.py, users/serializers.py

REASONING:
1. Hints point to 2 files - models.py and serializers.py
2. Problem is about inconsistent validation between model and serializer
3. Need to fix both files to ensure consistent validation

ACTIONS:
1. fetch_file_metadata("users/models.py")
   → Result: User model with email field
2. fetch_code_section("users/models.py", class_name="User")
   → Result: [User model code]
3. fetch_file_metadata("users/serializers.py")
   → Result: UserSerializer class
4. fetch_code_section("users/serializers.py", class_name="UserSerializer")
   → Result: [UserSerializer code]
5. generate_patch_with_qwen(
     problem="Add email validation to both model and serializer",
     code_section="[Combined code from both files]",
     file_path="users/models.py, users/serializers.py"
   )
   → Result: [Multi-file patch generated by Qwen]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

KEY PATTERNS:
1. Always use hints to guide your search
2. Fetch metadata before code (understand structure first)
3. Search for specific functions in large files
4. Fetch minimal focused code section
5. Always delegate to generate_patch_with_qwen (never generate yourself!)

Now, localize and fix this REAL problem:
"""


# ============================================================================
# QWEN SLM PROMPT (for patch generation)
# ============================================================================

QWEN_SYSTEM_PROMPT = """You are an expert code generator specializing in bug fixes. Your task is to generate a unified diff patch to fix bugs.

CRITICAL RULES:
1. Output ONLY the patch in unified diff format
2. Start directly with "--- a/" (no markdown, no explanation)
3. Make MINIMAL changes (only fix the bug)
4. Preserve all formatting, indentation, and whitespace
5. Include 3 lines of context before and after changes

UNIFIED DIFF FORMAT:
--- a/path/to/file.py
+++ b/path/to/file.py
@@ -start,count +start,count @@
 context line
 context line
-old line (removed)
+new line (added)
 context line
 context line

Generate the patch:
"""


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def fetch_file_metadata_impl(
    repo: str,
    commit: str,
    file_path: str
) -> Dict:
    """
    Fetch file metadata: size, functions, classes.
    
    For MVP, we'll do basic parsing. In production, use tree-sitter or AST.
    """
    try:
        # Fetch file content
        url = f"https://raw.githubusercontent.com/{repo}/{commit}/{file_path}"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        content = response.text
        lines = content.splitlines()
        
        # Basic parsing for Python files
        functions = []
        classes = []
        
        if file_path.endswith('.py'):
            for i, line in enumerate(lines):
                # Find function definitions
                if line.strip().startswith('def '):
                    func_name = line.strip().split('(')[0].replace('def ', '')
                    functions.append({
                        'name': func_name,
                        'line': i + 1
                    })
                # Find class definitions
                elif line.strip().startswith('class '):
                    class_name = line.strip().split('(')[0].split(':')[0].replace('class ', '')
                    classes.append({
                        'name': class_name,
                        'line': i + 1
                    })
        
        return {
            'file_path': file_path,
            'size': len(content),
            'lines': len(lines),
            'functions': functions,
            'classes': classes,
            'language': 'python' if file_path.endswith('.py') else 'unknown'
        }
    
    except Exception as e:
        return {
            'error': str(e),
            'file_path': file_path
        }


def search_codebase_impl(
    repo: str,
    commit: str,
    file_path: str,
    query: str
) -> List[Dict]:
    """
    Search for functions/classes matching query.
    
    For MVP, simple text search. In production, use semantic search.
    """
    try:
        metadata = fetch_file_metadata_impl(repo, commit, file_path)
        
        if 'error' in metadata:
            return []
        
        results = []
        query_lower = query.lower()
        
        # Search in functions
        for func in metadata['functions']:
            if query_lower in func['name'].lower():
                results.append({
                    'type': 'function',
                    'name': func['name'],
                    'line': func['line'],
                    'file_path': file_path
                })
        
        # Search in classes
        for cls in metadata['classes']:
            if query_lower in cls['name'].lower():
                results.append({
                    'type': 'class',
                    'name': cls['name'],
                    'line': cls['line'],
                    'file_path': file_path
                })
        
        return results
    
    except Exception as e:
        return []


def fetch_code_section_impl(
    repo: str,
    commit: str,
    file_path: str,
    start_line: Optional[int] = None,
    end_line: Optional[int] = None,
    function_name: Optional[str] = None,
    class_name: Optional[str] = None,
    context_lines: int = 10
) -> str:
    """
    Fetch specific code section from file.
    """
    try:
        url = f"https://raw.githubusercontent.com/{repo}/{commit}/{file_path}"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        lines = response.text.splitlines()
        
        # If function or class name provided, find it
        if function_name or class_name:
            target = f"def {function_name}" if function_name else f"class {class_name}"
            for i, line in enumerate(lines):
                if target in line:
                    start_line = max(1, i + 1 - context_lines)
                    # Find end of function/class (simple heuristic)
                    end_line = min(len(lines), i + 50)  # Assume ~50 lines
                    break
        
        # Default to provided line range
        if start_line is None:
            start_line = 1
        if end_line is None:
            end_line = len(lines)
        
        # Extract section
        section_lines = lines[start_line - 1:end_line]
        
        # Add line numbers
        numbered_lines = [
            f"{i + start_line:4d} | {line}"
            for i, line in enumerate(section_lines)
        ]
        
        return "\n".join(numbered_lines)
    
    except Exception as e:
        return f"Error fetching code: {e}"


def extract_patch_from_response(response: str) -> str:
    """Extract patch from LLM response."""
    # Remove markdown code blocks
    response = re.sub(r'```diff\n?', '', response)
    response = re.sub(r'```\n?', '', response)
    
    # Find the patch
    lines = response.split('\n')
    patch_lines = []
    in_patch = False
    
    for line in lines:
        if line.startswith('--- a/') or line.startswith('diff --git'):
            in_patch = True
        
        if in_patch:
            patch_lines.append(line)
    
    return '\n'.join(patch_lines).strip()


# ============================================================================
# MODAL FUNCTIONS
# ============================================================================

@app.cls(
    image=image,
    gpu="A100-40GB",
    timeout=600,
)
class QwenSLM:
    """Qwen SLM for patch generation."""
    
    @modal.enter()
    def load_model(self):
        """Load Qwen model on container startup."""
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        print(f"Loading {QWEN_MODEL}...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            QWEN_MODEL,
            trust_remote_code=True
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            QWEN_MODEL,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print(f"✅ Model loaded!")
    
    @modal.method()
    def generate_patch(
        self,
        problem: str,
        code_section: str,
        file_path: str
    ) -> Dict:
        """Generate patch using Qwen."""
        import torch
        
        start_time = time.time()
        
        # Build prompt
        user_prompt = f"""
PROBLEM:
{problem}

FILE: {file_path}
CODE SECTION:
```python
{code_section}
```

Generate the patch:
"""
        
        # Format for Qwen chat template
        messages = [
            {"role": "system", "content": QWEN_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ]
        
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # Generate
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=2048,
                temperature=0.2,
                top_p=0.95,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Decode
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        
        # Extract patch
        patch = extract_patch_from_response(generated_text)
        
        latency = time.time() - start_time
        
        return {
            'patch': patch,
            'latency': latency,
            'model': QWEN_MODEL
        }


@app.function(
    image=image,
    secrets=[modal.Secret.from_name("google-api-key")],
    timeout=600,
)
def generate_patch_phase2(problem: Dict) -> Dict:
    """
    Generate patch using Phase 2: Hint-based localization + LLM → SLM delegation.
    """
    import google.generativeai as genai
    
    start_time = time.time()
    
    # Configure Gemini
    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
    
    print(f"\n{'='*80}")
    print(f"Processing: {problem['instance_id']}")
    print(f"{'='*80}\n")
    
    # Track costs
    llm_latency = 0
    slm_latency = 0
    tool_calls = []
    
    # Define tools for Gemini
    def fetch_file_metadata_tool(file_path: str) -> str:
        """Get file metadata: size, functions, classes."""
        print(f"  🔧 Tool: fetch_file_metadata({file_path})")
        metadata = fetch_file_metadata_impl(
            repo=problem['repo'],
            commit=problem['base_commit'],
            file_path=file_path
        )
        result = json.dumps(metadata, indent=2)
        tool_calls.append({
            'tool': 'fetch_file_metadata',
            'args': {'file_path': file_path},
            'result_size': len(result)
        })
        return result
    
    def search_codebase_tool(file_path: str, query: str) -> str:
        """Search for functions/classes in file."""
        print(f"  🔧 Tool: search_codebase({file_path}, '{query}')")
        results = search_codebase_impl(
            repo=problem['repo'],
            commit=problem['base_commit'],
            file_path=file_path,
            query=query
        )
        result = json.dumps(results, indent=2)
        tool_calls.append({
            'tool': 'search_codebase',
            'args': {'file_path': file_path, 'query': query},
            'result_size': len(result)
        })
        return result
    
    def fetch_code_section_tool(
        file_path: str,
        start_line: Optional[int] = None,
        end_line: Optional[int] = None,
        function_name: Optional[str] = None
    ) -> str:
        """Fetch specific code section from file."""
        print(f"  🔧 Tool: fetch_code_section({file_path}, lines={start_line}-{end_line}, func={function_name})")
        code = fetch_code_section_impl(
            repo=problem['repo'],
            commit=problem['base_commit'],
            file_path=file_path,
            start_line=start_line,
            end_line=end_line,
            function_name=function_name
        )
        tool_calls.append({
            'tool': 'fetch_code_section',
            'args': {
                'file_path': file_path,
                'start_line': start_line,
                'end_line': end_line,
                'function_name': function_name
            },
            'result_size': len(code)
        })
        return code
    
    def generate_patch_with_qwen_tool(
        problem_description: str,
        code_section: str,
        file_path: str
    ) -> str:
        """Delegate patch generation to Qwen SLM."""
        nonlocal slm_latency
        print(f"  🤖 Tool: generate_patch_with_qwen (delegating to SLM...)")
        
        # Call Qwen SLM
        qwen = QwenSLM()
        result = qwen.generate_patch.remote(
            problem=problem_description,
            code_section=code_section,
            file_path=file_path
        )
        
        slm_latency += result['latency']
        
        tool_calls.append({
            'tool': 'generate_patch_with_qwen',
            'args': {
                'problem_description': problem_description[:100] + '...',
                'code_section_size': len(code_section),
                'file_path': file_path
            },
            'result_size': len(result['patch']),
            'slm_latency': result['latency']
        })
        
        return result['patch']
    
    # Create Gemini model with tools
    tools = [
        fetch_file_metadata_tool,
        search_codebase_tool,
        fetch_code_section_tool,
        generate_patch_with_qwen_tool
    ]
    
    model = genai.GenerativeModel(
        model_name="gemini-2.0-flash-exp",
        system_instruction=ROUTER_SYSTEM_PROMPT,
        tools=tools
    )
    
    # Start conversation
    chat = model.start_chat()
    
    user_message = f"""
PROBLEM STATEMENT:
{problem['problem_statement']}

REPOSITORY: {problem['repo']}

HINTS: {problem.get('hints_text', 'No hints provided')}

Localize the bug and generate a patch. Remember to delegate patch generation to generate_patch_with_qwen tool!
"""
    
    print("🧠 Router (Gemini) analyzing problem...")
    llm_start = time.time()
    response = chat.send_message(user_message)
    llm_latency += time.time() - llm_start
    
    # Handle tool calls
    max_turns = 10
    turn = 0
    final_patch = None
    
    while turn < max_turns:
        turn += 1
        
        # Check if we have function calls
        if not response.candidates or not response.candidates[0].content.parts:
            break
        
        function_calls = [
            part.function_call
            for part in response.candidates[0].content.parts
            if hasattr(part, 'function_call') and part.function_call
        ]
        
        if not function_calls:
            # No more function calls, check for final text
            text_parts = [
                part.text
                for part in response.candidates[0].content.parts
                if hasattr(part, 'text') and part.text
            ]
            if text_parts:
                final_patch = '\n'.join(text_parts)
            break
        
        # Execute function calls
        function_responses = []
        
        for fc in function_calls:
            try:
                # Get function and args
                func_name = fc.name
                args = dict(fc.args) if fc.args else {}
                
                print(f"\n  Turn {turn}: Router calls {func_name}")
                
                # Execute tool
                if func_name == "fetch_file_metadata_tool":
                    result = fetch_file_metadata_tool(**args)
                elif func_name == "search_codebase_tool":
                    result = search_codebase_tool(**args)
                elif func_name == "fetch_code_section_tool":
                    result = fetch_code_section_tool(**args)
                elif func_name == "generate_patch_with_qwen_tool":
                    result = generate_patch_with_qwen_tool(**args)
                    final_patch = result  # This is the final patch!
                else:
                    result = f"Unknown tool: {func_name}"
                
                function_responses.append(
                    genai.protos.Part(
                        function_response=genai.protos.FunctionResponse(
                            name=func_name,
                            response={'result': result}
                        )
                    )
                )
            
            except Exception as e:
                print(f"  ❌ Error executing {fc.name}: {e}")
                function_responses.append(
                    genai.protos.Part(
                        function_response=genai.protos.FunctionResponse(
                            name=fc.name,
                            response={'error': str(e)}
                        )
                    )
                )
        
        # Send function responses back to model
        if function_responses:
            llm_start = time.time()
            response = chat.send_message(function_responses)
            llm_latency += time.time() - llm_start
    
    total_latency = time.time() - start_time
    
    # Extract final patch
    if not final_patch:
        # Try to extract from last response
        if response.candidates and response.candidates[0].content.parts:
            text_parts = [
                part.text
                for part in response.candidates[0].content.parts
                if hasattr(part, 'text') and part.text
            ]
            if text_parts:
                final_patch = '\n'.join(text_parts)
    
    if final_patch:
        final_patch = extract_patch_from_response(final_patch)
    
    print(f"\n{'='*80}")
    print(f"✅ Completed in {total_latency:.2f}s")
    print(f"   LLM (Gemini): {llm_latency:.2f}s")
    print(f"   SLM (Qwen): {slm_latency:.2f}s")
    print(f"   Tool calls: {len(tool_calls)}")
    print(f"{'='*80}\n")
    
    return {
        "instance_id": problem['instance_id'],
        "repo": problem['repo'],
        "success": bool(final_patch),
        "completion": final_patch or "",
        "ground_truth_patch": problem.get('patch', ''),
        "localization": {
            "method": "hint_based",
            "hints": problem.get('hints_text', ''),
            "tool_calls": len(tool_calls),
            "tools_used": [tc['tool'] for tc in tool_calls]
        },
        "latency": total_latency,
        "llm_latency": llm_latency,
        "slm_latency": slm_latency,
        "tool_call_details": tool_calls,
        "strategy": "phase2_hint_based_delegation",
        "cost_breakdown": {
            "llm_cost_estimate": llm_latency * 0.01,  # Rough estimate
            "slm_cost_estimate": slm_latency * 0.002,  # Rough estimate
            "total_cost_estimate": (llm_latency * 0.01) + (slm_latency * 0.002)
        }
    }


@app.function(image=image, timeout=600)
def load_swebench_problem(instance_id: Optional[str] = None, index: int = 0):
    """Load a single problem from SWE-bench Lite."""
    from datasets import load_dataset
    
    print("Loading SWE-bench Lite dataset...")
    dataset = load_dataset("princeton-nlp/SWE-bench_Lite", split="test")
    
    if instance_id:
        for problem in dataset:
            if problem['instance_id'] == instance_id:
                return problem
        raise ValueError(f"Instance {instance_id} not found")
    else:
        return dataset[index]


@app.local_entrypoint()
def main(
    instance_id: Optional[str] = None,
    index: int = 0,
    save_local: bool = True
):
    """
    Run Phase 2: Hint-based localization with LLM → SLM delegation.
    
    Args:
        instance_id: Specific instance ID to test
        index: Dataset index if instance_id not provided
        save_local: Save results locally
    """
    print("\n" + "="*80)
    print("SWE-BENCH PHASE 2: HINT-BASED LOCALIZATION + DELEGATION")
    print("Strategy: Router (Gemini) → Localize → Delegate to SLM (Qwen)")
    print("="*80 + "\n")
    
    # Load problem
    if instance_id:
        print(f"Loading problem: {instance_id}")
    else:
        print(f"Loading problem at index: {index}")
    
    problem = load_swebench_problem.remote(instance_id=instance_id, index=index)
    
    print(f"\nProblem loaded:")
    print(f"  Instance ID: {problem['instance_id']}")
    print(f"  Repository: {problem['repo']}")
    print(f"  Hints: {problem.get('hints_text', 'No hints')}")
    print(f"  Problem: {problem['problem_statement'][:200]}...")
    print()
    
    # Generate patch
    result = generate_patch_phase2.remote(problem)
    
    # Print results
    print("\n" + "="*80)
    print("RESULTS")
    print("="*80 + "\n")
    
    if result['success']:
        print(f"✅ Success!")
        print(f"   Total Latency: {result['latency']:.2f}s")
        print(f"   LLM Latency: {result['llm_latency']:.2f}s (Gemini reasoning)")
        print(f"   SLM Latency: {result['slm_latency']:.2f}s (Qwen generation)")
        print(f"   Tool Calls: {result['localization']['tool_calls']}")
        print(f"   Tools Used: {', '.join(result['localization']['tools_used'])}")
        print()
        
        print(f"Cost Breakdown:")
        print(f"   LLM (Gemini): ~${result['cost_breakdown']['llm_cost_estimate']:.4f}")
        print(f"   SLM (Qwen): ~${result['cost_breakdown']['slm_cost_estimate']:.4f}")
        print(f"   Total: ~${result['cost_breakdown']['total_cost_estimate']:.4f}")
        print()
        
        print("Generated Patch:")
        print("-" * 80)
        print(result['completion'])
        print("-" * 80)
        print()
        
        print("Ground Truth Patch:")
        print("-" * 80)
        print(result['ground_truth_patch'])
        print("-" * 80)
        print()
        
        # Compare sizes
        gen_size = len(result['completion'])
        gt_size = len(result['ground_truth_patch'])
        ratio = gen_size / gt_size if gt_size > 0 else 0
        
        print(f"Patch Comparison:")
        print(f"  Generated: {gen_size:,} chars")
        print(f"  Ground Truth: {gt_size:,} chars")
        print(f"  Ratio: {ratio:.2f}x")
        
        if ratio < 3:
            print(f"  ✅ Good! Patch size is reasonable")
        else:
            print(f"  ⚠️  Warning: Patch is {ratio:.1f}x larger than ground truth")
    else:
        print(f"❌ Failed: No patch generated")
    
    # Save results
    if save_local:
        filename = f"results_phase2_{problem['instance_id']}.json"
        with open(filename, 'w') as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "strategy": "phase2_hint_based_delegation",
                "result": result
            }, f, indent=2)
        print(f"\n✅ Results saved to: {filename}")
    
    print("\n" + "="*80)
    print("DONE!")
    print("="*80 + "\n")

